{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we implement the Naive Bayes algorithm from scratch. We use the Scikit-Learn predefined functions of Naive Bayes and then compare the obtained results.\n",
    "First, we need to import the libraries we are going to use during the Naive Bayes implementation. We are going to use Numpy and Scikit-Learn libraries, if you haven't use them before, you need to first import them to the terminal using the pip command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the scratch implementation of Naive Bayes. You can go over the Scikit-Learn website for the detailed explanation: https://scikit-learn.org/stable/modules/naive_bayes.html. We use the logaritmic version of the equation to deal with additions instead of multiplications. If you want to deeper explanation for the scratch implementation, you can watch this tutorial: https://www.youtube.com/watch?v=TLInuAorxqE. The likelihood features are calculated using the Gaussian distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesScratch:\n",
    "\n",
    "    def initialize(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        self.mean = np.zeros((n_classes, n_features))\n",
    "        self.var = np.zeros((n_classes, n_features))\n",
    "        self.priors = np.zeros(n_classes)\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.mean[idx, :] = X_c.mean(axis=0)\n",
    "            self.var[idx, :] = X_c.var(axis=0)\n",
    "            self.priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self.fit(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def fit(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            posterior = np.sum(np.log(self.probabilityDensity(idx, x)))\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def probabilityDensity(self, idx, x):\n",
    "        mean = self.mean[idx]\n",
    "        var = self.var[idx]\n",
    "        probDen = (np.exp(-((x - mean) ** 2) / (2 * var))) / (np.sqrt(2 * np.pi * var))\n",
    "        return probDen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready! We can test our algorithm's performance using the well-known Iris dataset. This data sets consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray (src: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). We can easly import the dataset using the predefined Scikit-Learn function. In order to test the algorithm, we need to create separate train and test sets. We are going to use the 70% of our dataset to train the model and the rest will be used for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 30 points using our Naive Bayes Classifier: 1\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayesScratch()\n",
    "nb.initialize(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points using our Naive Bayes Classifier: %d\"\n",
    "       % (X_test.shape[0], (y_test != predictions).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model works really well! Let's compare the result with the predefined Scikit-Learn functions. There are 5 different Naive Bayes function: Gaussian NB, Multinominal NB, Complement NB, Bernoulli NB, and Categorical NB. Each of them has different advantages and disadvantages, and we need to understand our data to select the most suitable one in the given scenario. You can find the details in Scikitlearn's website https://scikit-learn.org/stable/modules/naive_bayes.html#. We select two of them for our application: Gaussian NB and Categorical NB. Since we also use the Gaussian approach in our implementation, the result of the two algorithms should be similar. Categorical NB assumes that each feature, which is described by the index , has its own categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 30 points using Gaussian Naive Bayes Classifier: 1\n",
      "Number of mislabeled points out of a total 30 points using Categorical Naive Bayes Classifier: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points using Gaussian Naive Bayes Classifier: %d\"\n",
    "       % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "gnb = CategoricalNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points using Categorical Naive Bayes Classifier: %d\"\n",
    "       % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
